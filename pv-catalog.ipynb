{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peculiar Velocity Catalogs and Models\n",
    "\n",
    "**Author:** Gabriel Henrique Souza Cardoso <sup>1 #</sup> <br/>\n",
    "<sup>1</sup> Eötvös Loránd University <br/>\n",
    "<sup>#</sup> gabrielhsc95@gmail.com <br/>\n",
    "\n",
    "***Last Update:*** 2020.04.15\n",
    "\n",
    "This notebook is to read and compare different catalogs and models of peculiar velocity. It is also creates a new catalog that matches the data used. If you have any question you can ask me at gabrielhsc95@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Since we are dealing with astronomy, a good library to handle units is Astropy (available at <link>https://www.astropy.org/</link>), it can be easily be installed using <code>pip</code>. \n",
    "\n",
    "It was created the function <code>cosmocalc_DL_Mpc</code> based on the package cosmocalc (<link>https://cxc.harvard.edu/contrib/cosmocalc/</link>) by Tom Aldcroft. It was made for Python 2, so it doesn't actually work properly in Python 3, but it easy to fix, you just need to fix the <code>print</code> function on line 253 in the source code just by adding the parentheses. It is also not very efficient, that's why it was created the function, so it is not necessary to install it.\n",
    "\n",
    "Some usual scientific libraries are used like numpy, pandas and matplotlib are also used, which are also available to install via <code>pip</code>.\n",
    "\n",
    "Other than that, some other standart libraries were used like csv, warnings and pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## review the necessary libraries\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "#from cosmocalc import cosmocalc\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import warnings\n",
    "import pickle\n",
    "from gabupy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "All the following code is dependend on two constants. The Hubble parameter ($ H_0 $) which can be set on the variable <code>H_0</code>, the value used is from the Planck Collaboration 2018 data release. The other constant is the uncertainty value for the peculiar velocity that is store in the variable <code>error_factor</code>, this number was infer in the **Gadget-2 Analysis Notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_0 = 67.66\n",
    "error_factor = 623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = [] # List that constains the Galaxy class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalogs\n",
    "\n",
    "First we are going to collec the data from the catalogs, after that, based on the position we will also see peculiar velocity value for the entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Mass Tully-Fisher Catalog (2MTF)\n",
    "\n",
    "The first catalog is the 2Mass Tully-Fisher\n",
    "\n",
    "Paper:  <link>https://arxiv.org/abs/1905.08530</link> or <link>https://academic.oup.com/mnras/article-abstract/487/2/2061/5497933<link>\n",
    "\n",
    "Data: <link>https://academic.oup.com/mnras/article/487/2/2061/5497933#supplementary-data</link> (It is the Table 2 that containts the peculiar velocity values)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the dat file.\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/2MTF/Table_2.dat', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 9):\n",
    "        next(reader)\n",
    "    \n",
    "    num = 0 # First unique number for the Galaxy class, index in the catalog\n",
    "    \n",
    "    for row in reader:\n",
    "        #Collect Data\n",
    "        name = row[0] # 2MASS\n",
    "        v_k = float(row[8]) # km/s\n",
    "        e_vk  = float(row[9]) # km/s\n",
    "        v_h  = float(row[10]) # km/s\n",
    "        e_vh = float(row[11]) # km/s\n",
    "        v_j   = float(row[12]) # km/s\n",
    "        e_vj  = float(row[13]) # km/s\n",
    "        wf_k  = float(row[14]) # km/s\n",
    "        e_wfk  = float(row[15]) # km/s\n",
    "        wf_h  = float(row[16]) # km/s\n",
    "        e_wfh  = float(row[17]) # km/s\n",
    "        wf_j  = float(row[18]) # km/s\n",
    "        e_wfj  = float(row[19]) # km/s\n",
    "        \n",
    "        \n",
    "        #Position\n",
    "        # hhmmssss -> deg\n",
    "        RA = float(row[0][0:2])*15.0 + float(row[0][2:4])*0.25 + float(row[0][4:8])*(360.0/8640000.0)\n",
    "        # ddmmsss - deg\n",
    "        dec =  float(row[0][9:11]) + float(row[0][11:13])/60.0 + float(row[0][13:16])/36000.0\n",
    "        if row[0][8:9] == \"-\":\n",
    "            dec *= -1.0\n",
    "        \n",
    "        g = Galaxy(num, name, None, RA, dec)\n",
    "        g.v2MTF_H = v_h\n",
    "        g.e2MTF_H = e_vh\n",
    "        g.v2MTF_J = v_j\n",
    "        g.e2MTF_J = e_vj\n",
    "        g.v2MTF_K = v_k\n",
    "        g.e2MTF_K = e_vk\n",
    "        g.v2MTF_WF15_H = wf_h\n",
    "        g.e2MTF_WF15_H = e_wfh\n",
    "        g.v2MTF_WF15_J = wf_j\n",
    "        g.e2MTF_WF15_J = e_wfj\n",
    "        g.v2MTF_WF15_K = e_wfk\n",
    "        g.e2MTF_WF15_K = wf_k\n",
    "        galaxies.append(g)\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warnings\n",
    "\n",
    "For the key 1046, which is\n",
    "\n",
    "| 2MASSname | cz_CMB | logd_km | err_km | logd_hm | err_hm | logd_jm | err_jm | v_k | e_vk | v_h | e_vh | v_j | e_vj | wf_k | e_wfk | wf_h | e_wfh | wf_j | e_wfj | flag |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 11550231+2717521 | 6922 | -0.0043 | 0.0958 | -0.0019 | 0.0932 | 0.0072 | 0.0971 | -38.57 | 8.53 | 0.01 | 0.00 | 143.57 | 32.07 | -68.54 | 1526.91 | -30.28 | 1485.47 | 114.76 | 1547.63 | -- |\n",
    "\n",
    "It will be ignored the velocity value <code>v_h</code> because its error value is zero (<code>e_vh</code>)\n",
    "\n",
    "\n",
    "The paper also raises attention to other two entries\n",
    "\n",
    "| 2MASSname | cz_CMB | logd_km | err_km | logd_hm | err_hm | logd_jm | err_jm | v_k | e_vk | v_h | e_vh | v_j | e_vj | wf_k | e_wfk | wf_h | e_wfh | wf_j | e_wfj | flag |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 09220265+5058353 | 812 | -0.3673 | 0.0863 | -0.3447 | 0.0808 | -0.3417 | 0.0858 | -1141.81 | 226.10 | -1043.11 | 194.55 | -1030.04 | 203.97 | -686.74 | 161.36 | -644.49 | 151.07 | -638.88 | 160.42 | -- |\n",
    "| 00383973+1724113 | 5082 | 0.4413 | 0.1429 | 0.4419 | 0.1498 | 0.4258 | 0.1499 | 3243.98 | 1068.15 | 3246.34 | 1121.25 | 3176.96 | 1097.28 | 5163.98 | 1672.18 | 5171.00 | 1752.92 | 4982.60 | 1754.09 | -- |\n",
    "\n",
    "The unique number generate in this work, 09220265+5058353 is 748 and 00383973+1724113 is 62.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match between 2MASS and \tPGC\n",
    "\n",
    "The first catalogs (2MTF) uses the 2MASS name, but the other ones have PGC name, so finding the name for matches is important because it is easier to combine by name not position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the match between the two catalog names by using GLADE, since it still stores the name of each catalog used. This first code just loads it in the memory, but it searches through the whole GLADE catalog. \n",
    "\n",
    "This is the whole macth between 2MASS and PGC, so it includes galaxies outside the range of the 2MTF catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was 26021 matches between 2MASS and PGC\n"
     ]
    }
   ],
   "source": [
    "with open('/media/gabriel/Data/University/ELTE/Thesis/GLADE/GLADE_2.3.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    matched = 0\n",
    "    PGC = []\n",
    "    TwoMASS = []\n",
    "    for row in reader:\n",
    "        if row[0] != 'null' and row[3] != 'null':\n",
    "            PGC.append(row[0])\n",
    "            TwoMASS.append(row[3])\n",
    "            matched += 1\n",
    "\n",
    "print(\"There was\", matched, \"matches between 2MASS and PGC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not load the whole GLADE catalog every single time, it is good to save the matching data, the next routine just saves it in a txt file where the first column is the name on PGC and the second column the equivalent in the 2MASS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/matching.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter='\\t', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for i in range(len(PGC)):\n",
    "        writer.writerow([PGC[i], TwoMASS[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the matching data computed from the previous two pieces of code, you can load it to the memory with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/matching.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    PGC = []\n",
    "    TwoMASS = []\n",
    "    for row in reader:\n",
    "        PGC.append(row[0])\n",
    "        TwoMASS.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the catalogs used the PGC name, it will be the main name used for this work, and the name will be used for matching all the catalog together, it is faster and easier than redo it by position.\n",
    "\n",
    "To find the galaxies afterwards where we did the name change, it will store (just in the memory) the unique id created in this work and new PGC name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of 1246 galaxies were changed for PGC\n"
     ]
    }
   ],
   "source": [
    "num_id = []\n",
    "PGC_id = []\n",
    "changed_names = 0\n",
    "\n",
    "for g in galaxies:\n",
    "    for i in range(len(PGC)):\n",
    "        if g.name == TwoMASS[i]:\n",
    "            g.name = PGC[i]\n",
    "            num_id.append(g.num)\n",
    "            PGC_id.append(PGC[i])\n",
    "            changed_names += 1\n",
    "\n",
    "print(\"The name of\", changed_names, \"galaxies were changed for PGC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosmicflows-3\n",
    "\n",
    "Paper: <link>https://arxiv.org/abs/1605.01765</link> or <link>https://iopscience.iop.org/article/10.3847/0004-6256/152/2/50</link>\n",
    "\n",
    "Data: <link>http://edd.ifa.hawaii.edu/dfirst.php</link> (it was download the whole table using tab as delimiter)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unique ID\n",
    "num = galaxies[-1].num + 1 # Last number + 1\n",
    "\n",
    "# Read the dat file.\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/CF-3.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 5):\n",
    "        next(reader)\n",
    "          \n",
    "    for row in reader:\n",
    "        #Collect data\n",
    "        name = row[0] # PGC\n",
    "        dist = float(row[1]) #Mpc\n",
    "        # hhmmss.s - > deg\n",
    "        RA = float(row[24][0:2])*15.0 + float(row[24][2:4])*0.25 + float(row[24][4:8])*(360.0/86400.0)\n",
    "        # ddmmss -> deg\n",
    "        dec = float(row[25][1:3]) + float(row[25][3:5])/60.0 + float(row[25][5:7])/360.0 #deg\n",
    "        if row[25][0:1] == \"-\":\n",
    "            dec *= -1.0\n",
    "        hubble_flow = H_0*dist\n",
    "        vel_cmb = float(row[37]) # km/s\n",
    "        vel_cmb_mod = float(row[38]) # km/s\n",
    "        vp = vel_cmb - hubble_flow\n",
    "        vp_mod = vel_cmb_mod - hubble_flow\n",
    "        \n",
    "        # Error\n",
    "        e_vp = error_factor\n",
    "        e_vp_mod = error_factor\n",
    "        \n",
    "        if name in PGC_id:\n",
    "            # Find the index of the PGC\n",
    "            index_name = PGC_id.index(name)\n",
    "            # Find the unique number assigned to this galaxy\n",
    "            index_id = num_id[index_name]\n",
    "            galaxies[index_id].vCF3 = vp\n",
    "            galaxies[index_id].eCF3 = e_vp\n",
    "            galaxies[index_id].vCF3_mod = vp_mod\n",
    "            galaxies[index_id].eCF3_mod = e_vp_mod\n",
    "            \n",
    "            if galaxies[index_id].dist == None:\n",
    "                galaxies[index_id].dist = dist\n",
    "        else:\n",
    "            g = Galaxy(num, name, dist, RA, dec)\n",
    "            g.vCF3 = vp\n",
    "            g.eCF3 = e_vp\n",
    "            g.vCF3_mod = vp_mod\n",
    "            g.eCF3_mod = e_vp_mod\n",
    "            galaxies.append(g)\n",
    "            # New PGC names\n",
    "            num_id.append(num)\n",
    "            PGC_id.append(name)\n",
    "            num += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6dFGS Catalog\n",
    "\n",
    "Paper: <link>https://academic.oup.com/mnras/article/445/3/2677/1036663</link> or <link>https://arxiv.org/abs/1409.6161</link>\n",
    "\n",
    "Data: <link>http://edd.ifa.hawaii.edu/dfirst.php</link> (it was download the whole table using tab as delimiter)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique ID\n",
    "num = galaxies[-1].num + 1\n",
    "\n",
    "# Read the dat file.\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/6dFGS_sample.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 5):\n",
    "        next(reader)\n",
    "          \n",
    "    for row in reader:\n",
    "        # Collect data\n",
    "        name = row[0] # PGC\n",
    "        v_cmb = float(row[14]) # km/s\n",
    "        dist = float(row[22]) #Mpc, it was used H_0 = 75\n",
    "        hubble_flow = H_0*dist\n",
    "        vp = v_cmb - hubble_flow\n",
    "        RA = float(row[38]) # deg\n",
    "        dec = float(row[39]) # deg\n",
    "        \n",
    "        # Error 100% (1.0)\n",
    "        e_vp = error_factor\n",
    "        \n",
    "        if name in PGC_id:\n",
    "            # Find the index of the PGC\n",
    "            index_name = PGC_id.index(name)\n",
    "            # Find the unique number assigned to this galaxy\n",
    "            index_id = num_id[index_name]\n",
    "            # Adjust for proper index in lists\n",
    "            galaxies[index_id].v6dFGS = vp\n",
    "            galaxies[index_id].e6dFGS = e_vp\n",
    "            \n",
    "            if galaxies[index_id].dist == None:\n",
    "                galaxies[index_id].dist = dist\n",
    "        else:\n",
    "            g = Galaxy(num, name, dist, RA, dec)\n",
    "            g.v6dFGS = vp\n",
    "            g.e6dFGS = e_vp\n",
    "            galaxies.append(g)\n",
    "            # New PGC names\n",
    "            num_id.append(num)\n",
    "            PGC_id.append(name)\n",
    "            num += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFI++ catalog\n",
    "\n",
    "Paper:\n",
    "\n",
    "Data: <link>http://edd.ifa.hawaii.edu/dfirst.php</link> (it was download the whole table using tab as delimiter)\n",
    "\n",
    "Distance in km/s\n",
    "\n",
    "$$ r_{\\text{group−malm}} = cz_{\\text{group}} − v_{\\text{group−malm}} $$\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique ID\n",
    "num = galaxies[-1].num + 1\n",
    "\n",
    "# Read the dat file.\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/SFIpp.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 5):\n",
    "        next(reader)\n",
    "          \n",
    "    for row in reader:\n",
    "        # Collect data\n",
    "        if row[29] != \"\": # If there is peculiar velocity data\n",
    "            name = row[0] # PGC\n",
    "            # hh mm ss.s -> deg\n",
    "            RA = float(row[5][1:3])*15.0 + float(row[5][4:6])*0.25 + float(row[5][7:11])*(360.0/86400.0)\n",
    "            # dd mm ss -> deg\n",
    "            dec = float(row[6][2:4]) + float(row[6][5:7])/60.0 + float(row[6][8:10])/360.0\n",
    "            if row[6][1:2] == \"-\":\n",
    "                dec *= -1.0\n",
    "            vp = float(row[29]) # km/s\n",
    "            e_vp = float(row[30]) # km/s\n",
    "            dist = float(row[31])/H_0  # km/s -> Mpc\n",
    "\n",
    "\n",
    "            if name in PGC_id:\n",
    "                # Find the index of the PGC\n",
    "                index_name = PGC_id.index(name)\n",
    "                # Find the unique number assigned to this galaxy\n",
    "                index_id = num_id[index_name]\n",
    "                # Adjust for proper index in lists\n",
    "                galaxies[index_id].vSFIpp = vp\n",
    "                galaxies[index_id].eSFIpp = e_vp\n",
    "                \n",
    "                if galaxies[index_id].dist == None:\n",
    "                    galaxies[index_id].dist = dist\n",
    "            else:\n",
    "                g = Galaxy(num, name, dist, RA, dec)\n",
    "                g.vSFIpp = vp\n",
    "                g.eSFIpp = e_vp\n",
    "                galaxies.append(g)\n",
    "                # New PGC names\n",
    "                num_id.append(num)\n",
    "                PGC_id.append(name)\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parnovsky RFGC peculiar velocities \n",
    "\n",
    "Paper:\n",
    "\n",
    "Data: <link>http://edd.ifa.hawaii.edu/dfirst.php</link> (it was download the whole table using tab as delimiter)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTANCE!!!!\n",
    "\n",
    "num = galaxies[-1].num + 1 # Unique number\n",
    "\n",
    "# Read the dat file.\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/RFGC.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 5):\n",
    "        next(reader)\n",
    "        \n",
    "    for row in reader:\n",
    "        name = row[0] # PGC\n",
    "        # hh mm ss.s -> deg\n",
    "        RA = float(row[4][1:3])*15.0 + float(row[4][4:6])*0.25 + float(row[4][7:11])*(360.0/86400.0)\n",
    "        # dd mm ss -> deg\n",
    "        dec = + float(row[5][2:4]) + float(row[5][5:7])/60.0 + float(row[5][8:10])/360.0\n",
    "        if row[5][1:2] == \"-\":\n",
    "            dec *= -1.0\n",
    "        dist_D = float(row[13])/H_0 # km/s -> Mpc\n",
    "        vp_D = float(row[17]) # km/s\n",
    "        dist_Q = float(row[18])/H_0 # km/s -> Mpc\n",
    "        vp_Q = float(row[23]) # km/s\n",
    "        dist_O = float(row[24])/H_0 # km/s -> Mpc\n",
    "        vp_O = float(row[30]) # km/s\n",
    "        \n",
    "        \n",
    "        # Error (100%)\n",
    "        e_vp_D = error_factor\n",
    "        e_vp_Q = error_factor\n",
    "        e_vp_O = error_factor\n",
    "        \n",
    "        if name in PGC_id:\n",
    "            # Find the index of the PGC\n",
    "            index_name = PGC_id.index(name)\n",
    "            # Find the unique number assigned to this galaxy\n",
    "            index_id = num_id[index_name]\n",
    "            # Adjust for proper index in lists\n",
    "            galaxies[index_id].vRFGC_D = vp_D\n",
    "            galaxies[index_id].eRFGC_D = e_vp_D\n",
    "            galaxies[index_id].vRFGC_Q = vp_Q\n",
    "            galaxies[index_id].eRFGC_Q = e_vp_Q\n",
    "            galaxies[index_id].vRFGC_O = vp_O\n",
    "            galaxies[index_id].eRFGC_O = e_vp_O\n",
    "            \n",
    "            if galaxies[index_id].dist == None:\n",
    "                galaxies[index_id].dist = dist_O\n",
    "        else:\n",
    "            g = Galaxy(num, name, dist_O, RA, dec)\n",
    "            g.vRFGC_D = vp_D\n",
    "            g.eRFGC_D = e_vp_D\n",
    "            g.vRFGC_Q = vp_Q\n",
    "            g.eRFGC_Q = e_vp_Q\n",
    "            g.vRFGC_O = vp_O\n",
    "            g.eRFGC_O = e_vp_O\n",
    "            galaxies.append(g)\n",
    "            num += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BORG model\n",
    "\n",
    "Paper: <link>https://arxiv.org/abs/1601.00093</link> or <link>https://iopscience.iop.org/article/10.1088/1475-7516/2017/06/049</link>\n",
    "\n",
    "Data: <link>http://www.florent-leclercq.eu/data.php</link>\n",
    "\n",
    "GitHub:<link>https://github.com/florent-leclercq/borg_sdss_data_release/blob/master/borg_sdss_velocity/borg_sdss_velocity.ipynb</link>\n",
    "\n",
    "It is a chrono-cosmographic analysis of the three-dimensional large-scale structure of the nearby Universe using data from the SDSS main sample galaxies. \n",
    "\n",
    "### Trilinear Interpolation\n",
    "\n",
    "TO DO - Demonstration in 2D\n",
    "\n",
    "In 3D,\n",
    "\n",
    "$$ v_{i0} = v(z_0,y_0,x_0)(x_1 - x) + v(z_0,y_0,x_1)(x - x_0) \\\\\n",
    "   v_{i1} = v(z_0,y_1,x_0)(x_1 - x) + v(z_0,y_1,x_1)(x - x_0) \\\\\n",
    "   v_{i2} = v(z_1,y_0,x_0)(x_1 - x) + v(z_1,y_0,x_1)(x - x_0) \\\\\n",
    "   v_{i3} = v(z_1,y_1,x_0)(x_1 - x) + v(z_1,y_1,x_1)(x - x_0) $$\n",
    "   \n",
    "$$ v_{i4} = v_{i0}(y_1 - y) + v_{i1}(y - y_0) \\\\\n",
    "   v_{i5} = v_{i2}(y_1 - y) + v_{i3}(y - y_0) $$\n",
    "   \n",
    "$$ \\bar{v_i} = v_{i4}(z_1 - z) + v_{i5}(z - z_0) $$\n",
    "\n",
    "The interpolation does not depend on the velocities itself, just on the distances. So it is not possible to do a propagation of error in the uncertanties. Therefore, the interpolation of the uncertanties is as same as the interpolation of the velocity values. However, the propagation is considered on the peculiar velocity.\n",
    "\n",
    "Distance is assume to be correct, the correction will be done by an interative loop until conversion to a single value.\n",
    "\n",
    "### Radial component\n",
    "\n",
    "$$ v_p = \\frac{\\bar{v_x}x + \\bar{v_y}y + \\bar{v_z}z}{r} $$\n",
    "\n",
    "$$ r = \\sqrt{x^2 + y^2 + z^2} $$\n",
    "\n",
    "$$ \\sigma_{v_p} = \\frac{\\sqrt{x^2 \\bar{\\sigma_{\\bar{v_x}}}^2 + y^2 \\bar{\\sigma_{\\bar{v_y}}}^2 + z^2 \\bar{\\sigma_{\\bar{v_z}}}^2}}{r}$$\n",
    "\n",
    "where $\\bar{\\alpha}$ is the interpolated value\n",
    "\n",
    "### Coordinates\n",
    "\n",
    "$$ x = \\frac{750}{256}i - 700 \\\\\n",
    "   y = \\frac{750}{256}j - 375 \\\\\n",
    "   z = \\frac{750}{256}k - 50 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using numpy\n",
    "velocity_BORG = np.load('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/borg_sdss_velocity.npz')\n",
    "\n",
    "#3D probabilistic maps for velocity field\n",
    "vx_mean=velocity_BORG['vx_mean']\n",
    "vx_var=velocity_BORG['vx_var']\n",
    "vy_mean=velocity_BORG['vy_mean']\n",
    "vy_var=velocity_BORG['vy_var']\n",
    "vz_mean=velocity_BORG['vz_mean']\n",
    "vz_var=velocity_BORG['vz_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 12927 galaxies outside of the range.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for g in galaxies:    \n",
    "    if g.dist == None:\n",
    "        dist = 1e42 # To make it outside the range\n",
    "    else:\n",
    "        dist = g.dist/(H_0/100.0) # Mpc/h\n",
    "        \n",
    "    coord = SkyCoord(ra=g.RA*u.degree, dec=g.dec*u.degree, distance=dist)\n",
    "    \n",
    "    x = coord.galactic.cartesian.x\n",
    "    y = coord.galactic.cartesian.y\n",
    "    z = coord.galactic.cartesian.z\n",
    "    \n",
    "    r = np.sqrt(np.square(x) + np.square(y) + np.square(z))\n",
    "    \n",
    "    i = (x + 700.0)*(256.0/750.0)\n",
    "    j = (y + 375.0)*(256.0/750.0)\n",
    "    k = (z + 50.0)*(256.0/750.0) \n",
    "    \n",
    "    # Interpolation based on the nearest points\n",
    "    #selecting the points in the grid\n",
    "    i0 = int(np.floor(i))\n",
    "    i1 = int(np.ceil(i))\n",
    "    j0 = int(np.floor(j))\n",
    "    j1 = int(np.ceil(j))\n",
    "    k0 = int(np.floor(k))\n",
    "    k1 = int(np.ceil(k))\n",
    "    \n",
    "    if(i1 <= 255 and j1 <= 255 and k1 <= 255 and i0 >= 0 and j0 >= 0 and k0 >= 0):\n",
    "        \"\"\"\n",
    "        #Analytically\n",
    "        #velecocity values\n",
    "        avg_vx = interpolate(vx_mean[k0,j0,i0], vx_mean[k0,j0,i1], vx_mean[k0,j1,i0], \n",
    "                             vx_mean[k0,j1,i1], vx_mean[k1,j0,i0], vx_mean[k1,j0,i1], \n",
    "                             vx_mean[k1,j1,i0], vx_mean[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                             j1, k, k0, k1)\n",
    "        avg_vy = interpolate(vy_mean[k0,j0,i0], vy_mean[k0,j0,i1], vy_mean[k0,j1,i0], \n",
    "                             vy_mean[k0,j1,i1], vy_mean[k1,j0,i0], vy_mean[k1,j0,i1], \n",
    "                             vy_mean[k1,j1,i0], vy_mean[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                             j1, k, k0, k1)\n",
    "        avg_vz = interpolate(vz_mean[k0,j0,i0], vz_mean[k0,j0,i1], vz_mean[k0,j1,i0], \n",
    "                             vz_mean[k0,j1,i1], vz_mean[k1,j0,i0], vz_mean[k1,j0,i1], \n",
    "                             vz_mean[k1,j1,i0], vz_mean[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                             j1, k, k0, k1)\n",
    "\n",
    "        # error values\n",
    "        # interpolate it as well\n",
    "        avg_vx_var = interpolate(vx_var[k0,j0,i0], vx_var[k0,j0,i1], vx_var[k0,j1,i0], \n",
    "                                 vx_var[k0,j1,i1], vx_var[k1,j0,i0], vx_var[k1,j0,i1], \n",
    "                                 vx_var[k1,j1,i0], vx_var[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                                 j1, k, k0, k1)\n",
    "        avg_vy_var = interpolate(vy_var[k0,j0,i0], vy_var[k0,j0,i1], vy_var[k0,j1,i0], \n",
    "                                 vy_var[k0,j1,i1], vy_var[k1,j0,i0], vy_var[k1,j0,i1], \n",
    "                                 vy_var[k1,j1,i0], vy_var[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                                 j1, k, k0, k1)\n",
    "        avg_vz_var = interpolate(vz_var[k0,j0,i0], vz_var[k0,j0,i1], vz_var[k0,j1,i0], \n",
    "                                 vz_var[k0,j1,i1], vz_var[k1,j0,i0], vz_var[k1,j0,i1], \n",
    "                                 vz_var[k1,j1,i0], vz_var[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                                 j1, k, k0, k1)\n",
    "\n",
    "        #peculiar velocity (radial direction)\n",
    "        vp = (avg_vx*x + avg_vy*y + avg_vz*z)/r\n",
    "        vp_e = np.sqrt(avg_vx_var*np.square(x) + \n",
    "                       avg_vy_var*np.square(y) + \n",
    "                       avg_vz_var*np.square(z))/r\n",
    "        \"\"\"\n",
    "        #Monte Carlo\n",
    "        vx_000 = np.random.normal(vx_mean[k0,j0,i0], np.sqrt(vx_var[k0,j0,i0]), 1000)\n",
    "        vx_001 = np.random.normal(vx_mean[k0,j0,i1], np.sqrt(vx_var[k0,j0,i1]), 1000)\n",
    "        vx_010 = np.random.normal(vx_mean[k0,j1,i0], np.sqrt(vx_var[k0,j1,i0]), 1000)\n",
    "        vx_011 = np.random.normal(vx_mean[k0,j1,i1], np.sqrt(vx_var[k0,j1,i1]), 1000)\n",
    "        vx_100 = np.random.normal(vx_mean[k1,j0,i0], np.sqrt(vx_var[k1,j0,i0]), 1000)\n",
    "        vx_101 = np.random.normal(vx_mean[k1,j0,i1], np.sqrt(vx_var[k1,j0,i1]), 1000)\n",
    "        vx_110 = np.random.normal(vx_mean[k1,j1,i0], np.sqrt(vx_var[k1,j1,i0]), 1000)\n",
    "        vx_111 = np.random.normal(vx_mean[k1,j1,i1], np.sqrt(vx_var[k1,j1,i1]), 1000)\n",
    "\n",
    "        vy_000 = np.random.normal(vy_mean[k0,j0,i0], np.sqrt(vy_var[k0,j0,i0]), 1000)\n",
    "        vy_001 = np.random.normal(vy_mean[k0,j0,i1], np.sqrt(vy_var[k0,j0,i1]), 1000)\n",
    "        vy_010 = np.random.normal(vy_mean[k0,j1,i0], np.sqrt(vy_var[k0,j1,i0]), 1000)\n",
    "        vy_011 = np.random.normal(vy_mean[k0,j1,i1], np.sqrt(vy_var[k0,j1,i1]), 1000)\n",
    "        vy_100 = np.random.normal(vy_mean[k1,j0,i0], np.sqrt(vy_var[k1,j0,i0]), 1000)\n",
    "        vy_101 = np.random.normal(vy_mean[k1,j0,i1], np.sqrt(vy_var[k1,j0,i1]), 1000)\n",
    "        vy_110 = np.random.normal(vy_mean[k1,j1,i0], np.sqrt(vy_var[k1,j1,i0]), 1000)\n",
    "        vy_111 = np.random.normal(vy_mean[k1,j1,i1], np.sqrt(vy_var[k1,j1,i1]), 1000)\n",
    "\n",
    "        vz_000 = np.random.normal(vz_mean[k0,j0,i0], np.sqrt(vz_var[k0,j0,i0]), 1000)\n",
    "        vz_001 = np.random.normal(vz_mean[k0,j0,i1], np.sqrt(vz_var[k0,j0,i1]), 1000)\n",
    "        vz_010 = np.random.normal(vz_mean[k0,j1,i0], np.sqrt(vz_var[k0,j1,i0]), 1000)\n",
    "        vz_011 = np.random.normal(vz_mean[k0,j1,i1], np.sqrt(vz_var[k0,j1,i1]), 1000)\n",
    "        vz_100 = np.random.normal(vz_mean[k1,j0,i0], np.sqrt(vz_var[k1,j0,i0]), 1000)\n",
    "        vz_101 = np.random.normal(vz_mean[k1,j0,i1], np.sqrt(vz_var[k1,j0,i1]), 1000)\n",
    "        vz_110 = np.random.normal(vz_mean[k1,j1,i0], np.sqrt(vz_var[k1,j1,i0]), 1000)\n",
    "        vz_111 = np.random.normal(vz_mean[k1,j1,i1], np.sqrt(vz_var[k1,j1,i1]), 1000)\n",
    "\n",
    "        sample_vx = interpolate(vx_000, vx_001, vx_010, vx_011, vx_100, vx_101, vx_110, vx_111,  \n",
    "                                i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "        sample_vy = interpolate(vy_000, vy_001, vy_010, vy_011, vy_100, vy_101, vy_110, vy_111,  \n",
    "                                i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "        sample_vz = interpolate(vz_000, vz_001, vz_010, vz_011, vz_100, vz_101, vz_110, vz_111,  \n",
    "                                i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "\n",
    "        sample_vp_mc = (sample_vx*x + sample_vy*y + sample_vz*z)/r\n",
    "        vp_mc = np.mean(sample_vp_mc)\n",
    "        vp_e_mc = np.std(sample_vp_mc)\n",
    "        \n",
    "        \n",
    "        g.vBORG = vp_mc\n",
    "        g.eBORG = vp_e_mc\n",
    "    else:\n",
    "        # print(\"Galaxy #\", g.num,  \"is outside the range of the grid\")\n",
    "        count += 1\n",
    "        \n",
    "           \n",
    "print(\"Total of\", count, \"galaxies outside of the range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warnings\n",
    "\n",
    "Total of 12725 galaxies outside of the range.\n",
    "\n",
    "The error propagation for the peculiar velovity is correct, but I am not sure if the interpolation of the error is valid.\n",
    "\n",
    "The error are normally order of magnitude bigger than the actual velocity, so it is not good for comparison or usage. BORG is not precise for peculiar velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrick\n",
    "\n",
    "Paper: <link>https://arxiv.org/abs/1504.04627</link> or <link>https://academic.oup.com/mnras/article/450/1/317/998219</link>\n",
    "\n",
    "Data: <link>http://cosmicflows.uwaterloo.ca/</link> or <link>https://cosmicflows.iap.fr/</link>\n",
    "\n",
    "The grid spacing is 1.5625 Mpc/h (400/256) and it does not contain uncertanties.\n",
    "\n",
    "Same sort of interpolation is used in this grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using numpy\n",
    "velocity_Carrick = np.load(\"/mnt/sda4/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/twompp_velocity.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100.4160944 , -130.25559066,   22.7990972 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velocity_Carrick[:,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 5591 galaxies outside of the range.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for g in galaxies:\n",
    "    if g.dist == None:\n",
    "        dist = 1e42 # To make it outside the range\n",
    "    else:\n",
    "        dist = g.dist/(H_0/100.0) # Mpc/h\n",
    "        \n",
    "    coord = SkyCoord(ra=g.RA*u.degree, dec=g.dec*u.degree, distance=dist)\n",
    "    \n",
    "    x = coord.galactic.cartesian.x\n",
    "    y = coord.galactic.cartesian.y\n",
    "    z = coord.galactic.cartesian.z\n",
    "    \n",
    "    r = np.sqrt(np.square(x) + np.square(y) + np.square(z))\n",
    "    \n",
    "    i = x*(256.0/400.0) + 128.0\n",
    "    j = y*(256.0/400.0) + 128.0\n",
    "    k = z*(256.0/400.0) + 128.0\n",
    "    \n",
    "    # Select nearest points in the grid\n",
    "    i0 = int(np.floor(i))\n",
    "    i1 = int(np.ceil(i))\n",
    "    j0 = int(np.floor(j))\n",
    "    j1 = int(np.ceil(j))\n",
    "    k0 = int(np.floor(k))\n",
    "    k1 = int(np.ceil(k))\n",
    "\n",
    "    if(i1 <= 255 and j1 <= 255 and k1 <= 255 and i0 >= 0 and j0 >= 0 and k0 >= 0):\n",
    "        #velecocity values\n",
    "        avg_v = interpolate(velocity_Carrick[:,i0,j0,k0], velocity_Carrick[:,i1,j0,k0], \n",
    "                            velocity_Carrick[:,i0,j1,k0], velocity_Carrick[:,i1,j1,k0], \n",
    "                            velocity_Carrick[:,i0,j0,k1], velocity_Carrick[:,i1,j0,k1], \n",
    "                            velocity_Carrick[:,i0,j1,k1], velocity_Carrick[:,i1,j1,k1], \n",
    "                            i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "\n",
    "        #peculiar velocity (radial direction)\n",
    "        vp = (avg_v[0]*x + avg_v[1]*y + avg_v[2]*z)/r\n",
    "        \n",
    "        #Error (100%)\n",
    "        e_vp = error_factor\n",
    "        \n",
    "        g.vCarrick = vp\n",
    "        g.eCarrick = e_vp\n",
    "    else:\n",
    "        # print(\"Galaxy #\", g.num,  \"is outside the range of the grid\")\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "print(\"Total of\", count, \"galaxies outside of the range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warnings\n",
    "\n",
    "There are 5549 galaxies out of the range, and there is no uncertanties.\n",
    "\n",
    "GLADE alredy uses it for redshift corrections, so just it by itself would be useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining values\n",
    "\n",
    "MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in galaxies:\n",
    "    vps = []\n",
    "    vps_e = []\n",
    "    if g.v2MTF_H != None and g.e2MTF_H != None and g.e2MTF_H != 0.0:\n",
    "        vps.append(g.v2MTF_H)\n",
    "        vps_e.append(g.e2MTF_H)\n",
    "    \n",
    "    if g.v2MTF_J != None and g.e2MTF_J != None and g.e2MTF_J != 0.0:\n",
    "        vps.append(g.v2MTF_J)\n",
    "        vps_e.append(g.e2MTF_J)\n",
    "    \n",
    "    if g.v2MTF_K != None and g.e2MTF_K != None and g.e2MTF_K != 0.0:\n",
    "        vps.append(g.v2MTF_K)\n",
    "        vps_e.append(g.e2MTF_K)\n",
    "    \n",
    "    if g.v2MTF_WF15_H != None and g.e2MTF_WF15_H != None and g.e2MTF_WF15_H != 0.0:\n",
    "        vps.append(g.v2MTF_WF15_H)\n",
    "        vps_e.append(g.e2MTF_WF15_H)\n",
    "        \n",
    "    if g.v2MTF_WF15_J != None and g.e2MTF_WF15_J != None and g.e2MTF_WF15_J != 0.0:\n",
    "        vps.append(g.v2MTF_WF15_J)\n",
    "        vps_e.append(g.e2MTF_WF15_J)\n",
    "        \n",
    "    if g.v2MTF_WF15_K != None and g.e2MTF_WF15_K != None and g.e2MTF_WF15_K != 0.0:\n",
    "        vps.append(g.v2MTF_WF15_K)\n",
    "        vps_e.append(g.e2MTF_WF15_K)\n",
    "        \n",
    "    if g.vCF3 != None and g.eCF3 != None and g.eCF3 != 0.0:\n",
    "        vps.append(g.vCF3)\n",
    "        vps_e.append(g.eCF3)\n",
    "        \n",
    "    if g.vCF3_mod != None and g.eCF3_mod != None and g.eCF3_mod != 0.0:\n",
    "        vps.append(g.vCF3_mod)\n",
    "        vps_e.append(g.eCF3_mod)\n",
    "        \n",
    "    if g.vSFIpp != None and g.eSFIpp != None and g.eSFIpp != 0.0:\n",
    "        vps.append(g.vSFIpp)\n",
    "        vps_e.append(g.eSFIpp)\n",
    "        \n",
    "    if g.v6dFGS != None and g.e6dFGS != None and g.e6dFGS != 0.0:\n",
    "        vps.append(g.v6dFGS)\n",
    "        vps_e.append(g.e6dFGS)\n",
    "        \n",
    "    if g.vRFGC_D != None and g.eRFGC_D != None and g.eRFGC_D != 0.0:\n",
    "        vps.append(g.vRFGC_D)\n",
    "        vps_e.append(g.eRFGC_D)\n",
    "    \n",
    "    if g.vRFGC_Q != None and g.eRFGC_Q != None and g.eRFGC_Q != 0.0:\n",
    "        vps.append(g.vRFGC_Q)\n",
    "        vps_e.append(g.eRFGC_Q)\n",
    "        \n",
    "    if g.vRFGC_O != None and g.eRFGC_O != None and g.eRFGC_O != 0.0:\n",
    "        vps.append(g.vRFGC_O)\n",
    "        vps_e.append(g.eRFGC_O)\n",
    "        \n",
    "    if g.vBORG != None and g.eBORG != None and g.eBORG != 0.0:\n",
    "        vps.append(g.vBORG)\n",
    "        vps_e.append(g.eBORG)\n",
    "        \n",
    "    if g.vCarrick != None and g.eCarrick != None and g.eCarrick != 0.0:\n",
    "        vps.append(g.vCarrick)\n",
    "        vps_e.append(g.eCarrick)\n",
    "        \n",
    "    if len(vps) != 0:\n",
    "        combined = mle(vps, vps_e)\n",
    "        g.vMLE = combined[0]\n",
    "        g.eMLE = combined[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the PV catalog\n",
    "\n",
    "Save catalog in a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNone(value):\n",
    "    if value == None:\n",
    "        value = \"null\"\n",
    "    return value\n",
    "\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/PVcatalog_v0.5.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter='\\t', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for g in galaxies:\n",
    "        writer.writerow([isNone(g.num), isNone(g.name), isNone(g.dist), isNone(g.RA), isNone(g.dec),\n",
    "                         isNone(g.v2MTF_H), isNone(g.e2MTF_H), isNone(g.v2MTF_J), \n",
    "                         isNone(g.e2MTF_J), isNone(g.v2MTF_K), isNone(g.e2MTF_K), isNone(g.v2MTF_WF15_H), \n",
    "                         isNone(g.e2MTF_WF15_H), isNone(g.v2MTF_WF15_J), isNone(g.e2MTF_WF15_J),\n",
    "                         isNone(g.v2MTF_WF15_K), isNone(g.e2MTF_WF15_K), isNone(g.vCF3), isNone(g.eCF3),\n",
    "                         isNone(g.vCF3_mod), isNone(g.eCF3_mod), isNone(g.vSFIpp), isNone(g.eSFIpp), \n",
    "                         isNone(g.v6dFGS), isNone(g.e6dFGS), isNone(g.vRFGC_D), isNone(g.eRFGC_D),\n",
    "                         isNone(g.vRFGC_Q), isNone(g.eRFGC_Q), isNone(g.vRFGC_O), isNone(g.eRFGC_O),\n",
    "                         isNone(g.vBORG), isNone(g.eBORG), isNone(g.vCarrick), isNone(g.eCarrick),\n",
    "                         isNone(g.vMLE), isNone(g.eMLE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/PVcatalog_v0.6.pickle', 'wb') as f:\n",
    "    pickle.dump(galaxies, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2MTF:\n",
      "max: 2595.22\n",
      "min: 0.0\n",
      "avg: 604.9929744584546\n",
      "\n",
      "SFI++:\n",
      "max: 10243.0\n",
      "min: 18.0\n",
      "avg: 1449.2264756730058\n",
      "\n",
      "BORG:\n",
      "max: 452.9518718563254\n",
      "min: 10.874404978035857\n",
      "avg: 198.05489077420017\n",
      "SFI++:\n",
      "max: 71.51976910815746\n",
      "min: 0.002118822134714631\n",
      "avg: 9.584216288129776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the dat file.\n",
    "errors_2MTF = []\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/2MTF/Table_2.dat', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 9):\n",
    "        next(reader)\n",
    "    \n",
    "    num = 1 # Unique number for the Galaxy class\n",
    "    \n",
    "    for row in reader:\n",
    "        #Collect Data\n",
    "        errors_2MTF.append(float(row[9]))\n",
    "        errors_2MTF.append(float(row[11]))\n",
    "        errors_2MTF.append(float(row[13]))\n",
    "        errors_2MTF.append(float(row[15]))\n",
    "        errors_2MTF.append(float(row[17]))\n",
    "        errors_2MTF.append(float(row[19]))\n",
    "\n",
    "print(\"2MTF:\")        \n",
    "print(\"max:\", max(errors_2MTF))\n",
    "print(\"min:\", min(errors_2MTF))\n",
    "print(\"avg:\", np.mean(errors_2MTF))\n",
    "print()\n",
    "\n",
    "\n",
    "errors_SFIpp = []\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/SFIpp.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "    \n",
    "    # Skip header\n",
    "    for i in range(0, 5):\n",
    "        next(reader)\n",
    "          \n",
    "    for row in reader:\n",
    "        # Collect data\n",
    "        if row[29] != \"\": # If there is peculiar velocity data\n",
    "            errors_SFIpp.append(float(row[30]))\n",
    "\n",
    "print(\"SFI++:\")        \n",
    "print(\"max:\", max(errors_SFIpp))\n",
    "print(\"min:\", min(errors_SFIpp))\n",
    "print(\"avg:\", np.mean(errors_SFIpp))\n",
    "print()\n",
    "\n",
    "errors_BORG = []\n",
    "# Load the data using numpy\n",
    "velocity_BORG = np.load('/media/gabriel/Data/University/ELTE/Thesis/Peculiar_Velocity_Catalogs/borg_sdss_velocity.npz')\n",
    "\n",
    "#3D probabilistic maps for velocity field\n",
    "vx_var=velocity_BORG['vx_var']\n",
    "vy_var=velocity_BORG['vy_var']\n",
    "vz_var=velocity_BORG['vz_var']\n",
    "\n",
    "for i in range(256):\n",
    "    print(i, end='\\r')\n",
    "    for j in range(256):\n",
    "        for k in range(256):\n",
    "            e_x = vx_var[k,j,i]\n",
    "            e_y = vy_var[k,j,i]\n",
    "            e_z = vz_var[k,j,i]\n",
    "            \n",
    "            x = (750./256.0)*i - 700.0\n",
    "            y = (750./256.0)*j - 375.0\n",
    "            z = (750./256.0)*k - 50.0\n",
    "            \n",
    "            r = np.sqrt(x*x + y*y + z*z)\n",
    "            \n",
    "            e_vp = np.sqrt(x*x*e_x + y*y*e_y + z*z*e_z)/r\n",
    "            errors_BORG.append(e_vp)\n",
    "            \n",
    "print(\"BORG:\")        \n",
    "print(\"max:\", max(errors_BORG))\n",
    "print(\"min:\", min(errors_BORG))\n",
    "print(\"avg:\", np.mean(errors_BORG))\n",
    "print()\n",
    "\n",
    "errors_GLADE = []\n",
    "with open('/media/gabriel/Data/University/ELTE/Thesis/GLADE/Calculated_Distances_3.txt', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t', quotechar='|', skipinitialspace=True)\n",
    "          \n",
    "    for row in reader:\n",
    "        # Collect data\n",
    "        if row[11] != 'null': # If there is peculiar velocity data\n",
    "            errors_GLADE.append(float(row[11]))\n",
    "\n",
    "print(\"GLADE:\")        \n",
    "print(\"max:\", max(errors_GLADE))\n",
    "print(\"min:\", min(errors_GLADE))\n",
    "print(\"avg:\", np.mean(errors_GLADE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.hist(errors_GLADE, bins=200, density=True)\n",
    "plt.ylabel(\"Frequency\", size='large')\n",
    "plt.xlabel(\"Uncertainty\", size='large')\n",
    "plt.ylim(0.0, 0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equality = [-750.0, 750.0]\n",
    "v1 = []\n",
    "e_v1 = []\n",
    "v2 = []\n",
    "e_v2 = []\n",
    "\n",
    "#for g in galaxies:\n",
    "for i in range(0,5):\n",
    "    g = galaxies[i]\n",
    "    v1.append(g.v2MTF)\n",
    "    e_v1.append(g.s2MTF)\n",
    "    v2.append(g.vBORG)\n",
    "    e_v2.append(g.sBORG)\n",
    "\n",
    "#e_v2 = np.zeros(len(v2))\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.errorbar(v1, v2, xerr = e_v1, yerr=e_v2, fmt='o')\n",
    "plt.errorbar(equality, equality, fmt='-')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((-500, 500)) \n",
    "#plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(galaxies[i].properties())\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(galaxies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(num_id)):\n",
    "    print(num_id[i], PGC_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = velocity_Carrick[:]\n",
    "print(v[:,42, 42, 42])\n",
    "print(velocity_Carrick[:,42,42,42])\n",
    "#print(velocity_Carrick[42,42,42])\n",
    "#print(v[42,42,42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_none = 0\n",
    "for g in galaxies:\n",
    "    if g.dist == None:\n",
    "        #print(g.num)\n",
    "        count_none += 1\n",
    "print(count_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies[1900].properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "for g in galaxies:\n",
    "    names.append(g.name)\n",
    "    \n",
    "print(len(names))\n",
    "\n",
    "unique = np.unique(names)\n",
    "print(unique.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = galaxies[42]\n",
    "SkyCoord(ra=g.RA*u.degree, dec=g.dec*u.degree, distance=1e42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vx_mean[0,0,255])\n",
    "print(vx_mean[0,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 255 <= 255:\n",
    "    print(\"oi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for times in range(20):\n",
    "    g_RA = np.random.random()*360.0\n",
    "    g_dec = (np.random.random()-0.5)*90.0\n",
    "    dist = np.random.random()*200.0\n",
    "\n",
    "    print(\"RA:\", g_RA, \"dec:\", g_dec, \"dist:\", dist)\n",
    "\n",
    "    coord = SkyCoord(ra=g_RA*u.degree, dec=g_dec*u.degree, distance=dist)\n",
    "\n",
    "    x = coord.galactic.cartesian.x\n",
    "    y = coord.galactic.cartesian.y\n",
    "    z = coord.galactic.cartesian.z\n",
    "\n",
    "\n",
    "    r = np.sqrt(np.square(x) + np.square(y) + np.square(z))\n",
    "\n",
    "    i = (x + 700.0)*(256.0/750.0)\n",
    "    j = (y + 375.0)*(256.0/750.0)\n",
    "    k = (z + 50.0)*(256.0/750.0) \n",
    "\n",
    "    # Interpolation based on the nearest points\n",
    "    #selecting the points in the grid\n",
    "    i0 = int(np.floor(i))\n",
    "    i1 = int(np.ceil(i))\n",
    "    j0 = int(np.floor(j))\n",
    "    j1 = int(np.ceil(j))\n",
    "    k0 = int(np.floor(k))\n",
    "    k1 = int(np.ceil(k))\n",
    "\n",
    "    if(i1 <= 255 and j1 <= 255 and k1 <= 255 and i0 >= 0 and j0 >= 0 and k0 >= 0):\n",
    "        print(\">> OK\")\n",
    "        #velecocity values\n",
    "        start_time = time.time()\n",
    "        avg_vx = interpolate(vx_mean[k0,j0,i0], vx_mean[k0,j0,i1], vx_mean[k0,j1,i0], \n",
    "                             vx_mean[k0,j1,i1], vx_mean[k1,j0,i0], vx_mean[k1,j0,i1], \n",
    "                             vx_mean[k1,j1,i0], vx_mean[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                             j1, k, k0, k1)\n",
    "        avg_vy = interpolate(vy_mean[k0,j0,i0], vy_mean[k0,j0,i1], vy_mean[k0,j1,i0], \n",
    "                             vy_mean[k0,j1,i1], vy_mean[k1,j0,i0], vy_mean[k1,j0,i1], \n",
    "                             vy_mean[k1,j1,i0], vy_mean[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                             j1, k, k0, k1)\n",
    "        avg_vz = interpolate(vz_mean[k0,j0,i0], vz_mean[k0,j0,i1], vz_mean[k0,j1,i0], \n",
    "                             vz_mean[k0,j1,i1], vz_mean[k1,j0,i0], vz_mean[k1,j0,i1], \n",
    "                             vz_mean[k1,j1,i0], vz_mean[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                             j1, k, k0, k1)\n",
    "\n",
    "        # error values\n",
    "        # interpolate it as well\n",
    "        avg_vx_var = interpolate(vx_var[k0,j0,i0], vx_var[k0,j0,i1], vx_var[k0,j1,i0], \n",
    "                                 vx_var[k0,j1,i1], vx_var[k1,j0,i0], vx_var[k1,j0,i1], \n",
    "                                 vx_var[k1,j1,i0], vx_var[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                                 j1, k, k0, k1)\n",
    "        avg_vy_var = interpolate(vy_var[k0,j0,i0], vy_var[k0,j0,i1], vy_var[k0,j1,i0], \n",
    "                                 vy_var[k0,j1,i1], vy_var[k1,j0,i0], vy_var[k1,j0,i1], \n",
    "                                 vy_var[k1,j1,i0], vy_var[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                                 j1, k, k0, k1)\n",
    "        avg_vz_var = interpolate(vz_var[k0,j0,i0], vz_var[k0,j0,i1], vz_var[k0,j1,i0], \n",
    "                                 vz_var[k0,j1,i1], vz_var[k1,j0,i0], vz_var[k1,j0,i1], \n",
    "                                 vz_var[k1,j1,i0], vz_var[k1,j1,i1], i, i0, i1, j, j0,\n",
    "                                 j1, k, k0, k1)\n",
    "\n",
    "        #peculiar velocity (radial direction)\n",
    "        vp = (avg_vx*x + avg_vy*y + avg_vz*z)/r\n",
    "        vp_e = np.sqrt(avg_vx_var*np.square(x) + \n",
    "                       avg_vy_var*np.square(y) + \n",
    "                       avg_vz_var*np.square(z))/r\n",
    "        t_1 = time.time() - start_time\n",
    "        \n",
    "        \n",
    "        #Monte Carlo\n",
    "        start_time = time.time()\n",
    "        vx_000 = np.random.normal(vx_mean[k0,j0,i0], np.sqrt(vx_var[k0,j0,i0]), 1000)\n",
    "        vx_001 = np.random.normal(vx_mean[k0,j0,i1], np.sqrt(vx_var[k0,j0,i1]), 1000)\n",
    "        vx_010 = np.random.normal(vx_mean[k0,j1,i0], np.sqrt(vx_var[k0,j1,i0]), 1000)\n",
    "        vx_011 = np.random.normal(vx_mean[k0,j1,i1], np.sqrt(vx_var[k0,j1,i1]), 1000)\n",
    "        vx_100 = np.random.normal(vx_mean[k1,j0,i0], np.sqrt(vx_var[k1,j0,i0]), 1000)\n",
    "        vx_101 = np.random.normal(vx_mean[k1,j0,i1], np.sqrt(vx_var[k1,j0,i1]), 1000)\n",
    "        vx_110 = np.random.normal(vx_mean[k1,j1,i0], np.sqrt(vx_var[k1,j1,i0]), 1000)\n",
    "        vx_111 = np.random.normal(vx_mean[k1,j1,i1], np.sqrt(vx_var[k1,j1,i1]), 1000)\n",
    "\n",
    "        vy_000 = np.random.normal(vy_mean[k0,j0,i0], np.sqrt(vy_var[k0,j0,i0]), 1000)\n",
    "        vy_001 = np.random.normal(vy_mean[k0,j0,i1], np.sqrt(vy_var[k0,j0,i1]), 1000)\n",
    "        vy_010 = np.random.normal(vy_mean[k0,j1,i0], np.sqrt(vy_var[k0,j1,i0]), 1000)\n",
    "        vy_011 = np.random.normal(vy_mean[k0,j1,i1], np.sqrt(vy_var[k0,j1,i1]), 1000)\n",
    "        vy_100 = np.random.normal(vy_mean[k1,j0,i0], np.sqrt(vy_var[k1,j0,i0]), 1000)\n",
    "        vy_101 = np.random.normal(vy_mean[k1,j0,i1], np.sqrt(vy_var[k1,j0,i1]), 1000)\n",
    "        vy_110 = np.random.normal(vy_mean[k1,j1,i0], np.sqrt(vy_var[k1,j1,i0]), 1000)\n",
    "        vy_111 = np.random.normal(vy_mean[k1,j1,i1], np.sqrt(vy_var[k1,j1,i1]), 1000)\n",
    "\n",
    "        vz_000 = np.random.normal(vz_mean[k0,j0,i0], np.sqrt(vz_var[k0,j0,i0]), 1000)\n",
    "        vz_001 = np.random.normal(vz_mean[k0,j0,i1], np.sqrt(vz_var[k0,j0,i1]), 1000)\n",
    "        vz_010 = np.random.normal(vz_mean[k0,j1,i0], np.sqrt(vz_var[k0,j1,i0]), 1000)\n",
    "        vz_011 = np.random.normal(vz_mean[k0,j1,i1], np.sqrt(vz_var[k0,j1,i1]), 1000)\n",
    "        vz_100 = np.random.normal(vz_mean[k1,j0,i0], np.sqrt(vz_var[k1,j0,i0]), 1000)\n",
    "        vz_101 = np.random.normal(vz_mean[k1,j0,i1], np.sqrt(vz_var[k1,j0,i1]), 1000)\n",
    "        vz_110 = np.random.normal(vz_mean[k1,j1,i0], np.sqrt(vz_var[k1,j1,i0]), 1000)\n",
    "        vz_111 = np.random.normal(vz_mean[k1,j1,i1], np.sqrt(vz_var[k1,j1,i1]), 1000)\n",
    "\n",
    "        sample_vx = interpolate(vx_000, vx_001, vx_010, vx_011, vx_100, vx_101, vx_110, vx_111,  \n",
    "                                i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "        sample_vy = interpolate(vy_000, vy_001, vy_010, vy_011, vy_100, vy_101, vy_110, vy_111,  \n",
    "                                i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "        sample_vz = interpolate(vz_000, vz_001, vz_010, vz_011, vz_100, vz_101, vz_110, vz_111,  \n",
    "                                i, i0, i1, j, j0, j1, k, k0, k1)\n",
    "\n",
    "        sample_vp_mc = (sample_vx*x + sample_vy*y + sample_vz*z)/r\n",
    "        vp_mc = np.mean(sample_vp_mc)\n",
    "        vp_e_mc = np.std(sample_vp_mc)\n",
    "        t_2 = time.time() - start_time\n",
    "\n",
    "\n",
    "        print(\"Interpolation:\", vp, \"±\", vp_e, \"in\", t_1, \"s\")\n",
    "        print(\"Monte Carlo:\", vp_mc, \"±\", vp_e_mc, \"in\", t_2, \"s\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\">> Outside of the range\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
